---
layout: default
---

<strong>Gradient descent</strong>

<img style="float: right; width: 450px;" src="/assets/img/grad_descent.png">

<p align="justify">
To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point.<br />
<a href="https://en.wikipedia.org/wiki/Gradient_descent"> source </a>
<br>
<a href="https://github.com/rasbt/python-machine-learning-book"> image source </a>
</p>

<br>

<strong>Stochastic gradient descent (SGD)</strong>
<p align="justify">
A stochastic approximation of the gradient descent for minimizing an objective function that is a sum of functions.
The true gradient is approximated by the gradient of a randomly chosen single function.<br />
<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent"> source </a>
</p>

<strong>Learning rate</strong>
<p align="justify">
The scalar by which the negative of the gradient is multiplied in gradient descent.
</p>

<strong>Backpropagation</strong>

<img style="float: left; width: 350px;" src="/assets/img/backprop.png">

<p align="justify">
An algorithm, relying on an iterative application of the chain rule, for computing efficiently the derivative of a neural network with respect to all of its parameters and feature vectors.<br />
<a href="https://en.wikipedia.org/wiki/Backpropagation"> source </a>
<br>
<a href="https://www.researchgate.net/figure/241741756_fig2_Figure-2-Back-propagation-multilayer-ANN-with-one-hidden-layer"> image source </a>
</p>

<br>
<br>
<br>
<br>

<strong>Goal function</strong>
<p align="justify">
The function being minimized in an optimization process, such as SGD.
</p>

<strong>Added noise</strong>

<img style="float: left; width: 250px;" src="/assets/img/noisy.png">

<p align="justify">
A perturbation added to the input of the network or one of the feature vectors it computes.
<br>
<a href="https://people.sc.fsu.edu/~jburkardt/m_src/image_denoise/image_denoise.html"> image source </a>
</p>

[back](cheat_sheet)
