---
layout: default
---

## Background
* [Emergence of simple cell](https://courses.cs.washington.edu/courses/cse528/11sp/Olshausen-nature-paper.pdf) by Olshausen and Field

## A list of readings for this course
1. Overview of the field from a practical point of view
- [ImageNet Classification with Deep Convolutional Neural Networks (Alexnet)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) by Krizhevsky et al.
- [Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG)](https://arxiv.org/abs/1409.1556) by Simonyan and Zisserman
- [Going Deeper with Convolutions (GoogLeNet)](https://arxiv.org/pdf/1409.4842.pdf) by Szegedy et al.
- [Deep Residual Learning for Image Recognition (ResNet)](https://arxiv.org/abs/1512.03385) by He et al.
- [Visualizing and Understanding Convolutional Neural Networks](https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf) by Zeiler and Fergus
- [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) by Kingma and Welling
- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) by Goodfellow et al.
2. [Understanding Deep Convolutional Networks](https://arxiv.org/pdf/1601.04920.pdf) by Mallat
3. [Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530) by Zhang et al.
4. [Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?](https://arxiv.org/abs/1504.08291) by Giryes et al.
5. [Robust Large Margin Deep Neural Networks](https://arxiv.org/abs/1605.08254) by Sokolic et al.
6. [Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems](https://arxiv.org/abs/1605.09232) by Giryes et al.
7. [Understanding Trainable Sparse Coding via Matrix Factorization](https://arxiv.org/pdf/1609.00285.pdf) by Moreau and Bruna
8. [Convolutional Neural Networks Analyzed via Convolutional Sparse Coding](https://arxiv.org/pdf/1607.08194.pdf) by Papyan et al.
9. [Why are Deep Nets Reversible: A Simple Theory, With Implications for Training](https://arxiv.org/pdf/1511.05653.pdf) by Arora et al.
10. [Stable Recovery of the Factors From a Deep Matrix Product and Application to Convolutional Network](https://arxiv.org/abs/1703.08044) by Malgouyres and Landsberg
11. [Optimal Approximation with Sparse Deep Neural Networks](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Kutyniok/Papers/OptApproxSparseDNN.pdf) by Bolcskei et al.

## Additional material
* [Learning Functions: When is Deep Better Than Shallow](https://arxiv.org/abs/1603.00988) by Mhaskar et al.
* [Convolutional Rectifier Networks as Generalized Tensor Decompositions](https://arxiv.org/abs/1603.00162) by Cohen and Shashua
* [A Probabilistic Theory of Deep Learning](https://arxiv.org/abs/1504.00641) by Patel et al.

[back](./)
