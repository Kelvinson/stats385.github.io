---
layout: default
---

## Lecture 1 -- Deep Learning Challenge. Is There Theory?
<ol>
    <li>
    <a href="https://sinews.siam.org/Details-Page/deep-deep-trouble"> Deep deep trouble </a>
    </li>
	<li>
    <a href="https://www.tune.com/blog/global-mobile-why-2016-is-the-global-tipping-point-for-the-mobile-economy/"> Why 2016 is the global tipping point... </a>
    </li>
	<li>
    <a href="http://www.digitalistmag.com/digital-economy/2017/07/19/ai-machine-learning-killing-analytics-as-we-know-it-05223779"> Are AI and ML killing analyticals... </a>
    </li>
	<li>
    <a href="https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/"> The dark secret at the heart of AI </a>
    </li>
	<li>
    <a href="http://www.independent.co.uk/life-style/gadgets-and-tech/news/ai-robots-artificial-intelligence-racism-sexism-prejudice-bias-language-learn-from-humans-a7683161.html"> AI robots learning racism... </a>
    </li>
	<li>
    <a href="https://www.theguardian.com/technology/2017/aug/10/faceapp-forced-to-pull-racist-filters-digital-blackface"> FaceApp forced to pull 'racist' filters... </a>
    </li>
	<li>
    <a href="http://nypost.com/2017/07/08/were-losing-a-whole-generation-of-young-men-to-video-games/"> Losing a whole generation of young men to video games </a>
    </li>
</ol>

## Lecture 2 -- Overview of Deep Learning From a Practical Point of View 
<ol>
	<li>
    <a href="https://courses.cs.washington.edu/courses/cse528/11sp/Olshausen-nature-paper.pdf"> Emergence of simple cell </a>
    </li>
    <li>
    <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"> ImageNet Classification with Deep Convolutional Neural Networks (Alexnet) </a>
    </li>
	<li>
    <a href="https://arxiv.org/abs/1409.1556"> Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG) </a>
    </li>
	<li>
    <a href="https://arxiv.org/pdf/1409.4842.pdf"> Going Deeper with Convolutions (GoogLeNet) </a>
    </li>
	<li>
    <a href="https://arxiv.org/abs/1512.03385"> Deep Residual Learning for Image Recognition (ResNet) </a>
    </li>
	<li>
    <a href="https://arxiv.org/pdf/1502.03167.pdf"> Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift </a>
    </li>
	<li>
    <a href="https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf"> Visualizing and Understanding Convolutional Neural Networks </a>
    </li>
</ol>

## Lecture 3
<ol>
    <li>
    <a href="https://www.nari.ee.ethz.ch/commth//pubs/files/deep-2016.pdf"> A mathematical theory of deep convolutional neural networks for feature extraction </a>
    </li>
    <li>
    <a href="https://www.nari.ee.ethz.ch/commth//pubs/files/Energy2017.pdf"> Energy propagation in deep convolutional neural networks </a>
    </li>
	<li>
	<a href="https://www.nari.ee.ethz.ch/commth//pubs/files/ICML2016.pdf"> Discrete deep feature extraction: A theory and new architectures </a>
	</li>
    <li>
    <a href="https://www.nari.ee.ethz.ch/commth//pubs/files/SPIE2017.pdf"> Topology reduction in deep convolutional feature extraction networks </a>
    </li>
</ol>


## To be discussed and extra
- [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) by Kingma and Welling
- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) by Goodfellow et al.
- [Understanding Deep Convolutional Networks](https://arxiv.org/pdf/1601.04920.pdf) by Mallat
- [Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530) by Zhang et al.
- [Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?](https://arxiv.org/abs/1504.08291) by Giryes et al.
- [Robust Large Margin Deep Neural Networks](https://arxiv.org/abs/1605.08254) by Sokolic et al.
- [Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems](https://arxiv.org/abs/1605.09232) by Giryes et al.
- [Understanding Trainable Sparse Coding via Matrix Factorization](https://arxiv.org/pdf/1609.00285.pdf) by Moreau and Bruna
- [Convolutional Neural Networks Analyzed via Convolutional Sparse Coding](https://arxiv.org/pdf/1607.08194.pdf) by Papyan et al.
- [Why are Deep Nets Reversible: A Simple Theory, With Implications for Training](https://arxiv.org/pdf/1511.05653.pdf) by Arora et al.
- [Stable Recovery of the Factors From a Deep Matrix Product and Application to Convolutional Network](https://arxiv.org/abs/1703.08044) by Malgouyres and Landsberg
- [Optimal Approximation with Sparse Deep Neural Networks](https://www.nari.ee.ethz.ch/commth//pubs/files/deep-approx-17.pdf) by Bolcskei et al.
- [Learning Functions: When is Deep Better Than Shallow](https://arxiv.org/abs/1603.00988) by Mhaskar et al.
- [Convolutional Rectifier Networks as Generalized Tensor Decompositions](https://arxiv.org/abs/1603.00162) by Cohen and Shashua
- [A Probabilistic Theory of Deep Learning](https://arxiv.org/abs/1504.00641) by Patel et al.

[back](./)
